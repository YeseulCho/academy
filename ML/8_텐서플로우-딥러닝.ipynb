{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network1 : XOR 문제와 학습방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR gate\n",
    "\n",
    "\n",
    "\n",
    "### or gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.37848285]\n",
      " [0.9092406 ]\n",
      " [0.88975763]\n",
      " [0.99252486]\n",
      " [0.91672   ]\n",
      " [0.99319214]\n",
      " [0.99450815]\n",
      " [0.99958354]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)\n",
    "# 하나만 '참'이여도 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 학습이 잘되었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.00846943]\n",
      " [0.04227257]\n",
      " [0.04183018]\n",
      " [0.18406567]\n",
      " [0.0395509 ]\n",
      " [0.17387232]\n",
      " [0.17545542]\n",
      " [0.5209716 ]] \n",
      "예측 :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "# 하나만 '거짓'이여도 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 학습이 잘 되었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.7499391 ]\n",
      " [0.7512872 ]\n",
      " [0.7494823 ]\n",
      " [0.7508321 ]\n",
      " [0.74915177]\n",
      " [0.748694  ]\n",
      " [0.75050277]\n",
      " [0.7500466 ]] \n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "# 서로 같은 값을 가질때만 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 학습이 잘안되여.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "x_data = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]] # 배열로 준비할 필요도 없음\n",
    "y_data = [0, 1, 1, 1, 1, 1, 1, 0] # 머신러닝에서는 1차원으로 넘겨주기\n",
    "\n",
    "clf = svm.SVC(C = 100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "examples = [[1, 1, 1], [1, 0, 1]]\n",
    "exam_labels = [0, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)\n",
    "\n",
    "score = metrics.accuracy_score(exam_labels, result)\n",
    "print(score)\n",
    "# 결과값 잘 나옴.. -> 인공신경망이 굳이 필요한가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.01406986]\n",
      " [0.99378496]\n",
      " [0.9956918 ]\n",
      " [0.9901768 ]\n",
      " [0.9942818 ]\n",
      " [0.98924005]\n",
      " [0.9915309 ]\n",
      " [0.02951473]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "# 안됐던 XOR 딥러닝 이용해보기\n",
    "# 그동안 하나만 가지고 했던 hidden 계층 여러개 사용\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# hidden layer1\n",
    "W1 = tf.Variable(tf.random_normal([3, 20]), tf.float32, name=\"weight\")\n",
    "    # 여기서 나오는 출력은 hidden layer2 로 넘어감\n",
    "    # 출력개수가 많을수록 학습량 증가, 시간 증가 \n",
    "b1 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "# hidden layer2\n",
    "W2 = tf.Variable(tf.random_normal([20, 1]), tf.float32, name=\"weight\")\n",
    "    # hidden layer1에서 출력 2개 시켜줬으므로 2를 입력 / hidden layer1과 2의 연결점 \n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) # layer1 전달받음  \n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 처음값과 마지막값의 가설% 낮음을 볼수있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[3.5730004e-04]\n",
      " [9.9980962e-01]\n",
      " [9.9976194e-01]\n",
      " [9.9963969e-01]\n",
      " [9.9975955e-01]\n",
      " [9.9979293e-01]\n",
      " [9.9985927e-01]\n",
      " [1.1996925e-03]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "# 연결횟수(hidden layer) 증가 = 와이드해진다\n",
    "# 깊게 만들고 싶다 = 계층의 개수 증가 시키기 = 패턴 더 잘 익히기\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "# hidden layer1\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "# hidden layer2\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2)+b2)\n",
    "\n",
    "# hidden layer3\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias3\")\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3)+b3)\n",
    "\n",
    "# hidden layer4\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias4\")\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4)+b4)\n",
    "\n",
    "# hidden layer5\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight5\")\n",
    "b5 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias5\")\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4, W5)+b5)\n",
    "\n",
    "# hidden layer6\n",
    "W6 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight6\")\n",
    "b6 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias6\")\n",
    "layer6 = tf.sigmoid(tf.matmul(layer5, W6)+b6)\n",
    "\n",
    "# hidden layer7\n",
    "W7 = tf.Variable(tf.random_normal([50, 1]), tf.float32, name=\"weight7\")\n",
    "b7 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias7\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer6, W7) + b7) \n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 100%로 잘 나옴\n",
    "    # 넓고 깊게 만들수록 좋은 성능을 갖음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서보드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.0181627 ]\n",
      " [0.9913869 ]\n",
      " [0.998994  ]\n",
      " [0.98902106]\n",
      " [0.99518085]\n",
      " [0.9873533 ]\n",
      " [0.9898292 ]\n",
      " [0.02885252]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.1 # 최소값 찾아갈때 보폭 0.1로하기\n",
    "\n",
    "tf.reset_default_graph() \n",
    "    # 기존에 있던 그래프들 지우고 새롭게 다시 시작할수있도록 해줌 / 자동 초기화\n",
    "    # clears the default graph stack and resets the global default graph\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "# layer1 그룹으로 묶어주기\n",
    "with tf.name_scope(\"layer1\") :\n",
    "    W1 = tf.Variable(tf.random_normal([3, 20]), tf.float32, name=\"weight\")\n",
    "    b1 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "    # 알고싶은 값 따로 지정\n",
    "    tf.summary.histogram(\"weight1\", W1) \n",
    "    tf.summary.histogram(\"bias1\", b1) \n",
    "    tf.summary.histogram(\"layer1\", layer1) \n",
    "    # 앞에 with에 들어있는 값만을 가지고 그려줌 # 결과를 어떤 그래프로 그릴것인지 지정 가능 # histogram : 가중치의 변화 알기\n",
    "    \n",
    "        \n",
    "# layer2 그룹으로 묶어주기\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([20, 1]), tf.float32, name=\"weight\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) \n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2) \n",
    "    tf.summary.histogram(\"bias2\", b2) \n",
    "    tf.summary.histogram(\"hypothesis\", hypothesis) \n",
    "\n",
    "    \n",
    "# cost 그룹으로 묶어주기 \n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    tf.summary.scalar(\"cost\", cost) # cost 변화량 보여주는 scalar 추가 \n",
    "    \n",
    "    \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy) # accuracy 값도 scalar로 보여줘!!\n",
    "\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    merged_summary = tf.summary.merge_all() # 지정한 값들을 하나로 합치자요\n",
    "    writer = tf.summary.FileWriter(\"log_dir5/alpha01\") # 지금까지 수집했던 파일들 폴더에 저장\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data}) # merge한것도 실행될수있게 list로 묶어서 넣어준다\n",
    "        writer.add_summary(summary, global_step=step) # 만번실행한거 계속 그래프로 그려주도록 # global_step : count training steps\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 처음값과 마지막값의 가설% 낮음을 볼수있다\n",
    "  \n",
    "\n",
    "# 도스로 가서\n",
    "# activate tf1\n",
    "# cd C:\\YS\\pythonwork\\AI     # log_dir5들어가있는 폴더 위치로 옮겨주기\n",
    "# tensorboard --logdir=./log_dir5/alpha01\n",
    "# 서버창 주소 알려줌\n",
    "# 주소 들어가서 graph 보장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.75896335]\n",
      " [0.7475103 ]\n",
      " [0.8149713 ]\n",
      " [0.7664579 ]\n",
      " [0.8574902 ]\n",
      " [0.8360547 ]\n",
      " [0.66411674]\n",
      " [0.5487796 ]] \n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph() \n",
    "    # 기존에 있던 그래프들 지우고 새롭게 다시 시작할수있도록 해줌 / 자동 초기화\n",
    "    # clears the default graph stack and resets the global default graph\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "# layer1 그룹으로 묶어주기\n",
    "with tf.name_scope(\"layer1\") :\n",
    "    W1 = tf.Variable(tf.random_normal([3, 20]), tf.float32, name=\"weight\")\n",
    "    b1 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "    # 알고싶은 값 따로 지정\n",
    "    tf.summary.histogram(\"weight1\", W1) \n",
    "    tf.summary.histogram(\"bias1\", b1) \n",
    "    tf.summary.histogram(\"layer1\", layer1) \n",
    "    # 앞에 with에 들어있는 값만을 가지고 그려줌 # 결과를 어떤 그래프로 그릴것인지 지정 가능 # histogram : 가중치의 변화 알기\n",
    "    \n",
    "        \n",
    "# layer2 그룹으로 묶어주기\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([20, 1]), tf.float32, name=\"weight\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) \n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2) \n",
    "    tf.summary.histogram(\"bias2\", b2) \n",
    "    tf.summary.histogram(\"hypothesis\", hypothesis) \n",
    "\n",
    "    \n",
    "# cost 그룹으로 묶어주기 \n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    tf.summary.scalar(\"cost\", cost) # cost 변화량 보여주는 scalar 추가 \n",
    "    \n",
    "    \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy) # accuracy 값도 scalar로 보여줘!!\n",
    "\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    merged_summary = tf.summary.merge_all() # 지정한 값들을 하나로 합치자요\n",
    "    writer = tf.summary.FileWriter(\"log_dir5/alpha001\") # 지금까지 수집했던 파일들 폴더에 저장\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data}) # merge한것도 실행될수있게 list로 묶어서 넣어준다\n",
    "        writer.add_summary(summary, global_step=step) # 만번실행한거 계속 그래프로 그려주도록 # global_step : count training steps\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "    # 처음값과 마지막값의 가설% 낮음을 볼수있다\n",
    "\n",
    "    \n",
    "# learning_rate=0.1, 0.01 의 graph 결과 같이 보고 싶다면 \n",
    "# 도스로 가서\n",
    "# activate tf1\n",
    "# cd C:\\YS\\pythonwork\\AI     # log_dir5들어가있는 폴더 위치로 옮겨주기\n",
    "# tensorboard --logdir=./log_dir5 까지만 입력    # 0.01의 값은 다른 폴더 alpha001에 저장해 두었으므로 \n",
    "# 서버창 주소 알려줌\n",
    "# 주소 들어가서 graph 보장\n",
    "\n",
    "# graph 모습을 통해서 어떤 learning_rate값을 가질때 더 좋은 성능을 갖는지 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network2 강의슬라이드 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network의 문제 : Vanishing gradient \n",
    " \n",
    " 계속적으로 미분하다보니 점점 0으로 수렴하게 됨\n",
    " \n",
    " layer가 많아지면 잘 작동하지 않게됨\n",
    " \n",
    " #### 해결방안 : ReLU(Rectified Linear Unit)\n",
    " \n",
    " \n",
    "                                 < ReLu >\n",
    "\n",
    "                                    y \n",
    "                                    |        /\n",
    "                                    |      /\n",
    "                                    |    /        ReLU\n",
    "                                    |  /\n",
    "                      ==============------------> x\n",
    "\n",
    "입력값이 0보다 작으면 0\n",
    "\n",
    "0보다 크면 입력값 그대로 내보낸다 \n",
    "\n",
    "비선형모델 해결\n",
    "\n",
    "### Neural Network의 문제 : 초기값  지정\n",
    "\n",
    "#### 해결방안 : Deep Belief Nets : RBM\n",
    "\n",
    "초기값 지정\n",
    "\n",
    "RBM : 초기값 찾는데 사용되는 알고리즘\n",
    "\n",
    "객체 두개씩 비교하여 가중치 값을 알고 거꾸로 진행하여 이전에 제시되었던 가중치 값과 비교한다\n",
    "\n",
    "가장 좋은 가중치 값을 학습시키는 훈련과정을 반복한다\n",
    "\n",
    "훈련 반복후, 가장 좋은 가중치의 초기값을 선택\n",
    "\n",
    "#### 해결방안 : Xavier/He initialization\n",
    "\n",
    "현재 입력값과 출력값을 입력값에 제곱근해서 2로 나눈 값으로 초기값을 구한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-c99e072dea65>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# 손글씨 데이터 불러오기\n",
    "\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 2.9054173868352717\n",
      "epoch : 2 cost : 1.1888215478983788\n",
      "epoch : 3 cost : 0.9351476798274302\n",
      "epoch : 4 cost : 0.8045783026651907\n",
      "epoch : 5 cost : 0.7273157743974165\n",
      "epoch : 6 cost : 0.6715535562688658\n",
      "epoch : 7 cost : 0.6303446601737628\n",
      "epoch : 8 cost : 0.5974287177215922\n",
      "epoch : 9 cost : 0.5711143242229121\n",
      "epoch : 10 cost : 0.5503080708330328\n",
      "epoch : 11 cost : 0.529526517716321\n",
      "epoch : 12 cost : 0.5135596744580702\n",
      "epoch : 13 cost : 0.5006760951063852\n",
      "epoch : 14 cost : 0.4877921371026473\n",
      "epoch : 15 cost : 0.4761292126503855\n",
      "epoch : 16 cost : 0.46581595323302527\n",
      "epoch : 17 cost : 0.45636968141252365\n",
      "epoch : 18 cost : 0.44785389499230815\n",
      "epoch : 19 cost : 0.44020044315945\n",
      "epoch : 20 cost : 0.43264112006534267\n",
      "epoch : 21 cost : 0.4253722293810413\n",
      "epoch : 22 cost : 0.4191695632175966\n",
      "epoch : 23 cost : 0.41288112076846\n",
      "epoch : 24 cost : 0.4089958102594721\n",
      "epoch : 25 cost : 0.4018903834711419\n",
      "epoch : 26 cost : 0.3979049494049767\n",
      "epoch : 27 cost : 0.39304926054044204\n",
      "epoch : 28 cost : 0.3889826542139051\n",
      "epoch : 29 cost : 0.38471928618170975\n",
      "epoch : 30 cost : 0.3800693345069887\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "# 기본\n",
    "# 가설 준비 작업\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# 그래프 작업\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypothesis =  tf.nn.softmax(logit)  # softmax : 10개중에 하나 선택해야하는 다중분류\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# 반복 돌리기\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.8999\n"
     ]
    }
   ],
   "source": [
    "# 기본\n",
    "# 정확도 알아보기 \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()\n",
    "\n",
    "# 기본 성능 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 1.8134565253691242\n",
      "epoch : 2 cost : 0.8003386311097582\n",
      "epoch : 3 cost : 0.6256853696432977\n",
      "epoch : 4 cost : 0.533401203697378\n",
      "epoch : 5 cost : 0.470671626329422\n",
      "epoch : 6 cost : 0.42444497428157113\n",
      "epoch : 7 cost : 0.3899482385678724\n",
      "epoch : 8 cost : 0.3597768449783322\n",
      "epoch : 9 cost : 0.33652678592638535\n",
      "epoch : 10 cost : 0.31657433070919716\n",
      "epoch : 11 cost : 0.2976300066167659\n",
      "epoch : 12 cost : 0.28323827738111657\n",
      "epoch : 13 cost : 0.2686315359852531\n",
      "epoch : 14 cost : 0.25682396688244563\n",
      "epoch : 15 cost : 0.2444315822016108\n",
      "epoch : 16 cost : 0.2338208798386834\n",
      "epoch : 17 cost : 0.22440454493869427\n",
      "epoch : 18 cost : 0.2156611261042679\n",
      "epoch : 19 cost : 0.20696783989667905\n",
      "epoch : 20 cost : 0.19968854481523665\n",
      "epoch : 21 cost : 0.19137575599280274\n",
      "epoch : 22 cost : 0.18416724015365935\n",
      "epoch : 23 cost : 0.17790192883123046\n",
      "epoch : 24 cost : 0.17213550009510736\n",
      "epoch : 25 cost : 0.16564391320401986\n",
      "epoch : 26 cost : 0.16067261056466542\n",
      "epoch : 27 cost : 0.15488395392894735\n",
      "epoch : 28 cost : 0.14987624891779644\n",
      "epoch : 29 cost : 0.14491978656161913\n",
      "epoch : 30 cost : 0.14062336276878012\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "# layer 3개 추가(wide)\n",
    "\n",
    "# 가설 준비 작업\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256])) # 256개로 출력개수 늘리기\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 =  tf.sigmoid(logit)  # 중간 layer들은 sigmoid로 바꿔보기\n",
    "\n",
    "# layer2\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 =  tf.sigmoid(logit) # 중간 layer들은 sigmoid로 바꿔보기\n",
    "\n",
    "# layer4\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.sigmoid(logit) # 중간 layer들은 sigmoid로 바꿔보기\n",
    "\n",
    "# 출력 layer3\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis =  tf.nn.softmax(logit) \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# 반복 돌리기\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.909\n"
     ]
    }
   ],
   "source": [
    "# layer 3개 추가(wide)\n",
    "# 정확도 알아보기 \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()\n",
    "\n",
    "# learning_rate = 0.2, layer 기본보다 3개 추가, activation function이 모두 softmax 일때, 정확도 27%로 기본 상태보다 성능이 떨어짐\n",
    "\n",
    "# 중간 activation function을 sigmoid로 바꿨을때 정확도는 90%로 성능이 좋아짐을 알수있다 \n",
    "# softmax는 마지막에만 사용해줘야한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 10.450723691853614\n",
      "epoch : 2 cost : 1.0620417768305004\n",
      "epoch : 3 cost : 0.872996025302193\n",
      "epoch : 4 cost : 0.7569866997545414\n",
      "epoch : 5 cost : 0.6694793382557956\n",
      "epoch : 6 cost : 0.611749583265998\n",
      "epoch : 7 cost : 0.5615520274639128\n",
      "epoch : 8 cost : 0.5239935722134333\n",
      "epoch : 9 cost : 0.49015272194688975\n",
      "epoch : 10 cost : 0.46132101871750547\n",
      "epoch : 11 cost : 0.4364852817492055\n",
      "epoch : 12 cost : 0.41555622902783484\n",
      "epoch : 13 cost : 0.40021233466538525\n",
      "epoch : 14 cost : 0.3805828995054418\n",
      "epoch : 15 cost : 0.36590437569401446\n",
      "epoch : 16 cost : 0.3539855955405668\n",
      "epoch : 17 cost : 0.3417034847086128\n",
      "epoch : 18 cost : 0.33154652324589834\n",
      "epoch : 19 cost : 0.321077783541246\n",
      "epoch : 20 cost : 0.31149947329000993\n",
      "epoch : 21 cost : 0.30584262392737643\n",
      "epoch : 22 cost : 0.2931645412878558\n",
      "epoch : 23 cost : 0.2869225306944415\n",
      "epoch : 24 cost : 0.2769659079746767\n",
      "epoch : 25 cost : 0.26904294084418917\n",
      "epoch : 26 cost : 0.2633162114024161\n",
      "epoch : 27 cost : 0.25996837475083084\n",
      "epoch : 28 cost : 0.2503146573088386\n",
      "epoch : 29 cost : 0.24552034383470356\n",
      "epoch : 30 cost : 0.24279207424684\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "# ReLu로 성능 바꿔보기 \n",
    "\n",
    "# 가설 준비 작업\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256])) # 256개로 출력개수 늘리기\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 =  tf.nn.relu(logit)  # relu\n",
    "\n",
    "# layer2\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 =  tf.sigmoid(logit) # sigmoid\n",
    "\n",
    "# layer4\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# 출력 layer3\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis =  tf.nn.softmax(logit) # softmax\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# 반복 돌리기\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9102\n"
     ]
    }
   ],
   "source": [
    "# ReLu\n",
    "# 정확도 알아보기 \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()\n",
    "\n",
    "# layer1, 3에 activation function을 relu로 이용하면 91% 성능을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 0.7272350878065282\n",
      "epoch : 2 cost : 0.20157400277527887\n",
      "epoch : 3 cost : 0.1399912708862261\n",
      "epoch : 4 cost : 0.1087356156178496\n",
      "epoch : 5 cost : 0.08676592341878202\n",
      "epoch : 6 cost : 0.07194143532352006\n",
      "epoch : 7 cost : 0.06034706205468285\n",
      "epoch : 8 cost : 0.05151024554602125\n",
      "epoch : 9 cost : 0.04242665168913927\n",
      "epoch : 10 cost : 0.036028052988377475\n",
      "epoch : 11 cost : 0.03087763718922029\n",
      "epoch : 12 cost : 0.02465764789672738\n",
      "epoch : 13 cost : 0.020555865641006026\n",
      "epoch : 14 cost : 0.01680362339893525\n",
      "epoch : 15 cost : 0.013549054021185088\n",
      "epoch : 16 cost : 0.01038968657778406\n",
      "epoch : 17 cost : 0.008453452683223247\n",
      "epoch : 18 cost : 0.0063848193705251235\n",
      "epoch : 19 cost : 0.005047177941834724\n",
      "epoch : 20 cost : 0.004152376162184573\n",
      "epoch : 21 cost : 0.0032058350697413782\n",
      "epoch : 22 cost : 0.002732307515107096\n",
      "epoch : 23 cost : 0.0023176085319243037\n",
      "epoch : 24 cost : 0.001960557277832941\n",
      "epoch : 25 cost : 0.0017610278445168994\n",
      "epoch : 26 cost : 0.0015833459566982275\n",
      "epoch : 27 cost : 0.001423974246612157\n",
      "epoch : 28 cost : 0.0013114310954485766\n",
      "epoch : 29 cost : 0.001133007957621224\n",
      "epoch : 30 cost : 0.0010097071370744905\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "# Xavier 초기화\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# 가설 준비 작업\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"Wr1\", shape =[784, 256], initializer=tf.contrib.layers.xavier_initializer()) # 초기화 랜덤하게 안함\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 =  tf.nn.relu(logit)  # relu\n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"Wr2\", shape =[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer3\n",
    "W3 = tf.get_variable(\"Wr3\", shape =[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit) # 중간 layer들은 sigmoid로 바꿔보기\n",
    "\n",
    "# 출력 layer4\n",
    "W4 = tf.get_variable(\"Wr4\", shape =[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis =  tf.nn.relu(logit) \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# 반복 돌리기\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9804\n"
     ]
    }
   ],
   "source": [
    "# ReLu\n",
    "# 정확도 알아보기 \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()\n",
    "\n",
    "# relu로 이용하면 98% 성능을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 2.072668454863809\n",
      "epoch : 2 cost : 0.6927107734029948\n",
      "epoch : 3 cost : 0.36363465000282635\n",
      "epoch : 4 cost : 0.255509789423509\n",
      "epoch : 5 cost : 0.20483567909760897\n",
      "epoch : 6 cost : 0.16377941621975467\n",
      "epoch : 7 cost : 0.13971356750889255\n",
      "epoch : 8 cost : 0.11831546080383379\n",
      "epoch : 9 cost : 0.109773821034892\n",
      "epoch : 10 cost : 0.15960125983438705\n",
      "epoch : 11 cost : 0.0806104143912142\n",
      "epoch : 12 cost : 0.06908337302505971\n",
      "epoch : 13 cost : 0.06153033190152867\n",
      "epoch : 14 cost : 0.0555542252585292\n",
      "epoch : 15 cost : 0.417295246683061\n",
      "epoch : 16 cost : 0.13299253805117173\n",
      "epoch : 17 cost : 0.05907504962587904\n",
      "epoch : 18 cost : 0.44519221190363184\n",
      "epoch : 19 cost : 0.12891232267022132\n",
      "epoch : 20 cost : 0.06668311413038856\n",
      "epoch : 21 cost : 0.04828208817507735\n",
      "epoch : 22 cost : 0.03795482586899941\n",
      "epoch : 23 cost : 0.0326773354495791\n",
      "epoch : 24 cost : 0.060912581871856386\n",
      "epoch : 25 cost : 0.022991682612760498\n",
      "epoch : 26 cost : 0.017976383473724145\n",
      "epoch : 27 cost : 0.01616901253709909\n",
      "epoch : 28 cost : 0.012907764926256447\n",
      "epoch : 29 cost : 0.009975228256715294\n",
      "epoch : 30 cost : 0.008219757394293661\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "# 깊이(deep) 8 layer, wide는 512\n",
    "\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# 가설 준비 작업\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"Wr1\", shape =[28*28, 512], initializer=tf.contrib.layers.xavier_initializer()) # 초기화 랜덤하게 안함\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 =  tf.nn.relu(logit)  # relu\n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"Wr2\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer3\n",
    "W3 = tf.get_variable(\"Wr3\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer4\n",
    "W4 = tf.get_variable(\"Wr4\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer5\n",
    "W5 = tf.get_variable(\"Wr5\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer6\n",
    "W6 = tf.get_variable(\"Wr6\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer7\n",
    "W7 = tf.get_variable(\"Wr7\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# layer8\n",
    "W8 = tf.get_variable(\"Wr8\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "layer8 =  tf.nn.relu(logit) # relu\n",
    "\n",
    "# 출력 layer9\n",
    "W9 = tf.get_variable(\"Wr9\", shape =[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer8, W9) + b9\n",
    "hypothesis =  tf.nn.relu(logit) \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# 반복 돌리기\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "# 시작값 좋음..왜 좋다는건지 음....이유는 나중에\n",
    "# 비용 잘 줄어듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.978\n"
     ]
    }
   ],
   "source": [
    "# 깊이(deep) 8 layer, wide는 512\n",
    "# 정확도 알아보기 \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()\n",
    "\n",
    "# deep, wide 시켰더니 성능 97.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 norm 방식 \n",
    "\n",
    "L2 reg = 람다 * tf.reduce_sum(tf.square(W))\n",
    "\n",
    "### Dropout\n",
    "\n",
    "과적합 문제 해결하기위한 기술\n",
    "\n",
    "훈련할때, 복잡게 모든것을 계산하지말고 랜덤하게 node를 빼어 계산\n",
    "\n",
    "남겨둘 node %로 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 1.1573737541653892\n",
      "epoch : 2 cost : 0.27189119883558965\n",
      "epoch : 3 cost : 0.20198865047909986\n",
      "epoch : 4 cost : 0.16054657803340386\n",
      "epoch : 5 cost : 0.1413048438050531\n",
      "epoch : 6 cost : 0.12732793595303185\n",
      "epoch : 7 cost : 0.1133448483659462\n",
      "epoch : 8 cost : 0.10620436086573375\n",
      "epoch : 9 cost : 0.09699841751632364\n",
      "epoch : 10 cost : 0.09053909363733097\n",
      "epoch : 11 cost : 0.08777642565017392\n",
      "epoch : 12 cost : 0.08506841149858455\n",
      "epoch : 13 cost : 0.07522750489921737\n",
      "epoch : 14 cost : 0.0719688410261138\n",
      "epoch : 15 cost : 0.07273618704216046\n",
      "epoch : 16 cost : 0.06812491935762494\n",
      "epoch : 17 cost : 0.062254680225795\n",
      "epoch : 18 cost : 0.06645197310738943\n",
      "epoch : 19 cost : 0.06128732596947385\n",
      "epoch : 20 cost : 0.06661509168926964\n",
      "epoch : 21 cost : 0.061768400291488904\n",
      "epoch : 22 cost : 0.05821525088426743\n",
      "epoch : 23 cost : 0.05626070771366358\n",
      "epoch : 24 cost : 0.0532013339075175\n",
      "epoch : 25 cost : 0.04806993313133717\n",
      "epoch : 26 cost : 0.05563479008322415\n",
      "epoch : 27 cost : 0.05256188796494493\n",
      "epoch : 28 cost : 0.060029130581427684\n",
      "epoch : 29 cost : 0.05358941474590792\n",
      "epoch : 30 cost : 0.05034234700893811\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "# dropout()\n",
    "\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# 가설 준비 작업\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "prob = tf.placeholder(tf.float32) # dropout\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"Wr1\", shape =[28*28, 512], initializer=tf.contrib.layers.xavier_initializer()) # 초기화 랜덤하게 안함\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 =  tf.nn.relu(logit) \n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=prob) # dropout \n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"Wr2\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 =  tf.nn.relu(logit) \n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=prob) # dropout\n",
    "\n",
    "# layer3\n",
    "W3 = tf.get_variable(\"Wr3\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit) \n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=prob) # dropout\n",
    "\n",
    "# layer4\n",
    "W4 = tf.get_variable(\"Wr4\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 =  tf.nn.relu(logit) \n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=prob) # dropout\n",
    "\n",
    "# layer5\n",
    "W5 = tf.get_variable(\"Wr5\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 =  tf.nn.relu(logit) \n",
    "layer5 = tf.nn.dropout(layer5, keep_prob=prob) # dropout\n",
    "\n",
    "# layer6\n",
    "W6 = tf.get_variable(\"Wr6\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 =  tf.nn.relu(logit) \n",
    "layer6 = tf.nn.dropout(layer6, keep_prob=prob) # dropout\n",
    "\n",
    "# layer7\n",
    "W7 = tf.get_variable(\"Wr7\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 =  tf.nn.relu(logit) \n",
    "layer7 = tf.nn.dropout(layer7, keep_prob=prob) # dropout \n",
    "\n",
    "# layer8\n",
    "W8 = tf.get_variable(\"Wr8\", shape =[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "layer8 =  tf.nn.relu(logit) \n",
    "layer8 = tf.nn.dropout(layer8, keep_prob=prob) # dropout\n",
    "\n",
    "# 출력 layer9\n",
    "W9 = tf.get_variable(\"Wr9\", shape =[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer8, W9) + b9\n",
    "hypothesis =  tf.nn.relu(logit) \n",
    "hypothesis = tf.nn.dropout(hypothesis, keep_prob=prob) # dropout \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# 반복 돌리기\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys, prob:0.7}) # dropout 70% node 남겨둘 거\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.98\n"
     ]
    }
   ],
   "source": [
    "# dropout()\n",
    "# 정확도 알아보기 \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1})) # 실제로 test할때는 100% 로 돌려놔야함\n",
    "sess.close()\n",
    "\n",
    "# learning_rate=0.1, dropout 시켰더니 성능 97.7% \n",
    "\n",
    "# learning_rate=0.01, dropout 시켰더니 성능 86.6%  \n",
    "\n",
    "# learning_rate=0.001, dropout, AdamOptimizer 시켰더니 성능 98% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "\n",
    "가장 좋은 방식을 투표(voting)\n",
    "\n",
    "soft voting : 하나의 확률로 통합하여 처리 \n",
    "\n",
    "hard voting : 각 모델에게 투표용지를 따로따로 주어 가장 높은값을 처리\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 가장 기본적인 알고리즘\n",
    "\n",
    "### CNN(Convolutional Neural Network)  ★면접에 잘나와용★\n",
    "\n",
    "Fully Connected layer\n",
    "\n",
    "filter를 이용하여 이미지의 부분적 특징을 뽑아오도록 = Convolultion\n",
    "\n",
    "7 * 7 이미지(N)를  3 * 3 filter(F)로  1칸씩(stride 움직이는 간격) 이동하여  특징을 뽑아내면 5 * 5 의 결과를 가져온다 \n",
    "\n",
    "Output size = (N - F) / stride + 1\n",
    "\n",
    "#### padding\n",
    "\n",
    "주어진 이미지의 테두리에 0을 채워넣는 것\n",
    "\n",
    "장점 : 이미지 크기 보존, 이미지의 경계선 표시 \n",
    "\n",
    "7 * 7 이미지(N)에 패딩을 채우고  3 * 3 filter(F)로  1칸씩(stride 움직이는 간격) 이동하여  특징을 뽑아내면 7 * 7 의 결과를 가져온다 \n",
    "\n",
    "\n",
    "필터에 의해 만들어진 layer\n",
    "\n",
    "필터 종류의 개수(가중치값)에 따라 convolution layer 생성\n",
    "\n",
    "convolution layer에 이미지의 중요한 특징들을 담아낸다 = 전처리\n",
    "\n",
    "convolution layer에서 뽑아낸 특징들을 또 뽑아내어 convolution layer 다시 만들어냄\n",
    "\n",
    "#### max pooling \n",
    "\n",
    "쌓여있는 데이터를 인공신경망(Neural Network)에 넘겨서 \n",
    "\n",
    "sdkfj;ksejfaiejfasjf\n",
    "\n",
    "하나의 이미지의 filter\n",
    "\n",
    "\n",
    "### RNN  ★면접에 잘나와용★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
