{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "rnn.pdf / lec12.pdf / Lab12-RNN.pdf 참고\n",
    "\n",
    "ex)\n",
    "\n",
    "    I google at work\n",
    "\n",
    "    I work at google\n",
    "\n",
    "    google, work 단어의 생김새는 동일하지만 품사는 다름\n",
    "\n",
    "    앞뒤 단어의 뜻에 따라 달라지는 품사\n",
    "\n",
    "    RNN에서는 이것을 인식시킬수있다.(CNN에서는 못함)\n",
    "\n",
    "\n",
    "RNN은 전체적인 맥락을 잡아내고 문장을 만들어낼수있다 \n",
    "\n",
    "### Sequence data\n",
    "\n",
    "자신한테서 출력된 값이 자신한테 입력되는 순환구조를 이룸\n",
    "\n",
    "#### 출력 방향 2가지\n",
    "\n",
    "    : output\n",
    "        \n",
    "        : 결과값 출력 \n",
    "        \n",
    "        : 다항분류에 사용   ex) 제목, 문장 만들기 \n",
    "\n",
    "    : state\n",
    "    \n",
    "        : 옆에 있는 layer에게 영향을 주는 결과값 출력\n",
    "\n",
    "        : 이전 값을 계속적으로 고려하여 계산을 진행해나갈수있다\n",
    "\n",
    "        : 안에서 값을 변형해가면서 학습하기\n",
    "        \n",
    "        : 마지막에 나오는 state는 최종 결과값이 될 수 있다\n",
    "        \n",
    "        : 이진분류 중 감정분류 시에 많이 사용   ex) 행복한가, 부정적인가 긍정적인가\n",
    "        \n",
    "        \n",
    "#### ex) hihello\n",
    "\n",
    "입력데이터 : hihell\n",
    "\n",
    "출력데이터 : ihello\n",
    "\n",
    "같은 h 인데 그 뒤에 i가 올지 e가 올지 어떻게 예측하는가? 그 앞뒤를 보자\n",
    "\n",
    "\n",
    "#### POS tagging\n",
    "\n",
    "품사 구분 : output 을 보고서 명사, 형용사 구분을 학습시킴\n",
    "\n",
    "가중치 계산에서는 옆 layer에서 넘어온 Wx 값도 같이 계산해줘야함 -> h = tanh(W1h0 + W2x2)\n",
    "\n",
    "#### Sentiment Analysis\n",
    "\n",
    "행복 분류를 state 최종 결과 보고 정하기\n",
    "\n",
    "bptt(back propagation through time) : 시간에 따른 back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanila RNN\n",
    "기본 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]]\n",
      "----------------------------------------------\n",
      "[[[ 0.1084308   0.23005372]\n",
      "  [-0.44041133 -0.59082305]\n",
      "  [-0.46746534  0.61441123]\n",
      "  [-0.5914525   0.64826185]\n",
      "  [-0.28519422 -0.5536942 ]]]\n",
      "----------------------------------------------\n",
      "[[-0.28519422 -0.5536942 ]]\n"
     ]
    }
   ],
   "source": [
    "# hello 입력해서 출력하기 \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 입력값 준비 : hello\n",
    "# one hot encoding : 숫자로 바꿔줘야 입력이 가능함\n",
    "\n",
    "h = [1, 0, 0, 0] # 중복된거 빼면 4글자\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# 셀 준비\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=2) #hidden size 2개인 이유는...??? 01이라서?# 두개로 잡았으니 결과 2개로 나옴\n",
    "\n",
    "# 입력데이터 준비\n",
    "x_data = np.array([[h, e, l, l, o]], dtype=np.float32) \n",
    "    # sequence의 길이가 됨  # 입력데이터 총 3차원(문자열 최대 차원)\n",
    "print(x_data)\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "# rnn 실행\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32) \n",
    "                                    # 실행시킬거, 입력데이터, type  # 2개의 결과 변수로 선언\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(sess.run(outputs)) # 각각의 시퀀스에 따른 결과 \n",
    "                         # h, e, l. l. o 순으로 나온 결과값\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(sess.run(_state))\n",
    "# 마지막으로 나온 output 과 state의 결과값은 같아야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.20793891  0.24406303 -0.75278705]\n",
      "  [-0.06346128 -0.52844936  0.68356085]\n",
      "  [-0.36491966  0.8857268  -0.02324395]]\n",
      "\n",
      " [[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.30707452  0.62735885  0.21719742]\n",
      "  [ 0.5043804  -0.14038289  0.3744523 ]\n",
      "  [-0.11641277  0.70696247 -0.7512605 ]]]\n",
      "------------------------------------------------\n",
      "[[-0.36491966  0.8857268  -0.02324395]\n",
      " [-0.11641277  0.70696247 -0.7512605 ]]\n",
      "------------------------------------------------\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(7, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# 품사 구분하도록 입력하기\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# 입력값 준비\n",
    "# one hot encoding 시키기\n",
    "# 문장1 : I work at google = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "# 문장2 : I google at work = [[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]\n",
    "\n",
    "\n",
    "# 입력데이터 준비 : 문자 데이터 3차원으로 만들기\n",
    "inputs = np.array([[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n",
    "                  [[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]]) # 각각의 데이터를 3차원으로 만들어주기\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "\n",
    "# 셀준비\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=3) # 몇개로 출력하는지에 대해 지정하는지에 따라서 음.. 중요하진 않아요^^\n",
    "\n",
    "# rnn 실행\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, inputs=tf_inputs, dtype=tf.float32) \n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(sess.run(outputs))\n",
    "# 문장1과 문장2 비교\n",
    "# I  : 처음 시작값 똑같이 나옴 이니까\n",
    "# at : 앞(과거)에서부터 받은 가중치 값이 다르므로 다른 값을 가짐 \n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(sess.run(_state))\n",
    "# 각각의 state값은 outputs의 마지막 출력값과 동일\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 가중치 값에 따라 shape이 어떻게 잡히는지 알아보기....??\n",
    "variable_names = [v.name for v in tf.trainable_variables()]\n",
    "for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(v)\n",
    "                # 가중치 값까지 알아볼 필요는 없어용.........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.638345        prediction: [[0 3 3 3 3 3 2]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "1 loss: 1.386576        prediction: [[0 0 0 3 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "2 loss: 1.1635329        prediction: [[0 0 0 3 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "3 loss: 0.93181574        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "4 loss: 0.70485556        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "5 loss: 0.50350326        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "6 loss: 0.3518529        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "7 loss: 0.2512133        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "8 loss: 0.18412565        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "9 loss: 0.13794506        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "10 loss: 0.10591277        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "11 loss: 0.08293753        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "12 loss: 0.06509359        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "13 loss: 0.05109658        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "14 loss: 0.0404573        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "15 loss: 0.032309774        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "16 loss: 0.026021114        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "17 loss: 0.021202112        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "18 loss: 0.017518586        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "19 loss: 0.014667387        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "20 loss: 0.012415267        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "21 loss: 0.010607043        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "22 loss: 0.009145096        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "23 loss: 0.00796453        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "24 loss: 0.0070157093        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "25 loss: 0.0062551987        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "26 loss: 0.0056438395        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "27 loss: 0.005147829        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "28 loss: 0.004740025        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "29 loss: 0.0043999692        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "30 loss: 0.0041122227        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "31 loss: 0.0038651435        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "32 loss: 0.0036498925        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "33 loss: 0.0034596974        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "34 loss: 0.0032894185        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "35 loss: 0.003135436        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "36 loss: 0.0029953595        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "37 loss: 0.0028672724        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "38 loss: 0.0027501176        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "39 loss: 0.002642904        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "40 loss: 0.0025446217        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "41 loss: 0.0024545991        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "42 loss: 0.00237189        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "43 loss: 0.002295922        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "44 loss: 0.0022258833        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "45 loss: 0.0021610637        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "46 loss: 0.0021010917        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "47 loss: 0.002045341        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "48 loss: 0.0019934902        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "49 loss: 0.001945133        prediction: [[0 1 0 2 3 3 4]]        true Y :  [[0, 1, 0, 2, 3, 3, 4]]\n",
      "\n",
      " 결과 : h, i, h, e, l, l, o\n"
     ]
    }
   ],
   "source": [
    "# hihello 학습 과정\n",
    "# h가 입력되면 예측되는 값은 i / i가 입력되면 예측되는 값은 h ..로 나와야한다\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# 중복된 단어빼고 숫자화 시키기\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# one-hot encoding\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],       # h 0  \n",
    "              [0, 1, 0, 0, 0],       # i 1 \n",
    "              [1, 0, 0, 0, 0],       # h 0\n",
    "              [0, 0, 1, 0, 0],       # e 2\n",
    "              [0, 0, 0, 1, 0],       # l 3\n",
    "              [0, 0, 0, 1, 0],       # l 3\n",
    "              [0, 0, 0, 0, 1]]]      # 0 4\n",
    "\n",
    "# 입력 데이터\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]\n",
    "          # h, i, h, e, l, l\n",
    "\n",
    "# 출력 데이터\n",
    "y_data = [[0, 1, 0, 2, 3, 3, 4]]\n",
    "         # h, i, h, e, l, l, o \n",
    "    \n",
    "# X, y 변수 준비\n",
    "    # [batch_size(입력값의 차원의 개수), sequence length, input dimension]\n",
    "    # 각각의 값 변수에 담기 \n",
    "    # [batch_size(입력값의 차원의 개수), sequence length, input dimension] \n",
    "    # hi끼리 묶고, ih끼리 묶고 등등.. 이렇게 나눠서 입력하지 않으므로 batch size는 1임\n",
    "    # hihell 6개 넣고 ihello 6개 나오니 sequence length 6\n",
    "    # 중복된 글자 빼고 h, i, e, l, o -> input dimension 5\n",
    "    \n",
    "num_classes = 5\n",
    "input_dim = 5 \n",
    "hidden_size = 5 # num_units 와 같은 뜻\n",
    "batch_size = 1\n",
    "sequence_len = 7\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, sequence_len, input_dim]) # batch_size 부분 무슨값 None으로 처리 \n",
    "y = tf.placeholder(tf.int32, [None, sequence_len]) # 결과 입력한 개수만큼 나옴\n",
    "\n",
    "# rnn 모델 작성\n",
    "## 셀준비\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) \n",
    "## 초기값 지정\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)  # cell안에있는 각각의 값을 0으로 청소해주기\n",
    "## rnn 실행\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32,\n",
    "                                   initial_state=initial_state) # 초기값 넣어줘서 더 깨끗한 값 만들어내자요오오오옹\n",
    "\n",
    "# 평면화\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size]) \n",
    "            # 2차원으로 만들어주기위해 reshape 사용 / flatten은 걍 1차원으로 만들어버리니까\n",
    "    \n",
    "# FC\n",
    "## W, b, logit 준비하기 / 한줄에 작성해주는 함수 \n",
    "outputs = tf.contrib.layers.fully_connected(inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "                                            # 입력값, 출력개수, activation_function = None 안쓸꺼야....\n",
    "    \n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_len, num_classes])\n",
    "# activation function을 통과시키지 않은 output이 logit인데 비용계산 부분에 넣을떄는 3차원으로 넣어야한다 \n",
    "# 그렇기에 크기 재조정해줘야한다\n",
    "weights = tf.ones([batch_size, sequence_len])\n",
    "# 가중치 다 1로 채워서 넘겨준다 왜?.......ㅎ\n",
    "\n",
    "# 비용 계산\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = y, weights = weights) \n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# 예측값\n",
    "prediction = tf.argmax(outputs, 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 훈련횟수\n",
    "    for i in range(50):\n",
    "        _, cost = sess.run([train, loss], feed_dict={X:x_one_hot, y:y_data})\n",
    "        result = sess.run(prediction, feed_dict={X:x_one_hot})\n",
    "        print(i, \"loss:\", cost, \"       prediction:\", result, \"       true Y : \", y_data)\n",
    "        # 비용이 잘 줄어드는가 # 예측값과 실제값이 잘 맞는가 / 한 3번째부터 맞기시작한것을 볼수있음\n",
    "        \n",
    "    # 결과값 문자로 출력해보자\n",
    "    result_str = [idx2char[c] for c in np.squeeze(result)] # np.squeeze : 현재 차원에서 한 차원줄이는 함수\n",
    "    print(\"\\n 결과 :\", \", \".join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient vanishing : 값이 너무 작아져 측정할 수가 없음\n",
    "\n",
    "#### gradient exploding : 값이 커져버리면 수렴이 안됨 최저비용에 도착을 못함\n",
    "\n",
    "\n",
    "이것들을 고안해서 나온 방법▼\n",
    "\n",
    "### LSTM(long sequence training memory)\n",
    "\n",
    "새로운 state가 하나 더 출력이 된다 전첵적인 출력의 개수는 3 <- Vanila와의 차이점\n",
    "\n",
    "forget gate : 새로운 기억이 들어갈 곳을 남겨두기 위해 잔여 데이터 기억을 몇% 지운다\n",
    "\n",
    "하나의 memory cell에는 과거의 기억과 새로운 기억이 같이 존재할 수 있도록 한다\n",
    "\n",
    "memory cell : 기억해야할 것들을 따로 보관하는 형태\n",
    "\n",
    "두가지 상태선을 가지고 처리한당용ㄻㅇ란이런아ㅏ\n",
    "\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output values\n",
      "[[[0.09927537]]]\n",
      "\n",
      "memory cell value \n",
      "[[0.18134572]]\n",
      "\n",
      "hidden state value \n",
      "[[0.09927537]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "inputs = np.array([[[1, 0]]])\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=1)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=tf_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_run, states_run = sess.run([outputs, _states])\n",
    "    print(\"output values\")\n",
    "    print(outputs_run)\n",
    "    print(\"\\nmemory cell value \")\n",
    "    print(states_run.c)\n",
    "    print(\"\\nhidden state value \")\n",
    "    print(states_run.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category\n",
       "0   dishplace is located in sunnyvale downtown the...     food\n",
       "1   service can be slower during busy hours but ou...     food\n",
       "2   portions are huge both french toast and their ...     food\n",
       "3   we started with apps going the chicken and waf...     food\n",
       "4   the biscuits and gravy was too salty two peopl...     food\n",
       "5   the garlic fries were a great starter (and a h...     food\n",
       "6   our meal was excellent i had the pasta ai form...     food\n",
       "7   what i enjoy most about palo alto is so many r...     food\n",
       "8   the drinks came out fairly quickly a good two ...     food\n",
       "9   despite the not so good burger the service was...     food\n",
       "10  the four reigning major champions simona halep...   sports\n",
       "11  the briton was seeded nn7 here last year befor...   sports\n",
       "12  stephens surged her way back from injury in st...   sports\n",
       "13  when it came to england chances in the world c...   sports\n",
       "14  the team that eliminated russia – croatia – al...   sports\n",
       "15  the perseyside outfit finished in fourth place...   sports\n",
       "16  liverpool fc will return to premier league act...   sports\n",
       "17  alisson signed for liverpool fc from as roma t...   sports\n",
       "18  but the rankings during that run-in to new yor...   sports\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_dict_list = [\n",
    "         {'paragraph': 'dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "         {'paragraph': 'service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "         {'paragraph': 'portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "         {'paragraph': 'we started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "         {'paragraph': 'the biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "         {'paragraph': 'the garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "         {'paragraph': 'our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "         {'paragraph': 'what i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "         {'paragraph': 'the drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "         {'paragraph': 'despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "    \n",
    "         {'paragraph': 'the four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "         {'paragraph': 'the briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "         {'paragraph': 'stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "         {'paragraph': 'when it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "         {'paragraph': 'the team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "         {'paragraph': 'the perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "         {'paragraph': 'liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "         {'paragraph': 'alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "         {'paragraph': 'but the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "         {'paragraph': 'then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the thiadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df = df[[\"paragraph\", \"category\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = set()\n",
    "df[\"paragraph\"].str.lower().str.split().apply(results.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(and',\n",
       " '1',\n",
       " '1.5',\n",
       " '1.5x',\n",
       " '10-9',\n",
       " '14',\n",
       " '2',\n",
       " '2014',\n",
       " '33/1',\n",
       " 'a',\n",
       " 'about',\n",
       " 'action',\n",
       " 'after',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ai',\n",
       " 'alisson',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'and',\n",
       " 'angelique',\n",
       " 'any',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'area',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'australia',\n",
       " 'avoided',\n",
       " 'azarenka',\n",
       " 'back',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'beef',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'began—too',\n",
       " 'being',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'biscuits',\n",
       " 'bit',\n",
       " 'bookmark',\n",
       " 'bookmarked',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'breakthroughs',\n",
       " 'briton',\n",
       " 'brunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'campaigns',\n",
       " 'can',\n",
       " 'card',\n",
       " 'caroline',\n",
       " 'case',\n",
       " 'certainly',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chicken',\n",
       " 'city',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'commented',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'confidence',\n",
       " 'contrast',\n",
       " 'could',\n",
       " 'course',\n",
       " 'courteous',\n",
       " 'courts',\n",
       " 'coworkers',\n",
       " 'crisp',\n",
       " 'croatia',\n",
       " 'crumbs',\n",
       " 'culminating',\n",
       " 'cup',\n",
       " 'currently',\n",
       " 'dare',\n",
       " 'days',\n",
       " 'decisive',\n",
       " 'defending',\n",
       " 'delivered',\n",
       " 'despite',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'dish',\n",
       " 'dishplace',\n",
       " 'dismal',\n",
       " 'displayed',\n",
       " 'djokovic',\n",
       " 'djokovic-nadal',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'dog-friendly',\n",
       " 'dominance',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'down',\n",
       " 'downtown',\n",
       " 'dragged',\n",
       " 'drinks',\n",
       " 'during',\n",
       " 'eat',\n",
       " 'edged',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'eliminated',\n",
       " 'enduring',\n",
       " 'england',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enormously',\n",
       " 'entree',\n",
       " 'error',\n",
       " 'especially',\n",
       " 'even',\n",
       " 'event',\n",
       " 'excellent',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'extremely',\n",
       " 'face',\n",
       " 'failed',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fans',\n",
       " 'fc',\n",
       " 'fellow',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'first-round',\n",
       " 'five',\n",
       " 'flair',\n",
       " 'flavors',\n",
       " 'flight',\n",
       " 'food',\n",
       " 'for',\n",
       " 'form',\n",
       " 'formaggi',\n",
       " 'four',\n",
       " 'fourth',\n",
       " 'french',\n",
       " 'fries',\n",
       " 'from',\n",
       " 'full',\n",
       " 'games',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'generous',\n",
       " 'goalkeeper',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gravy',\n",
       " 'great',\n",
       " 'green',\n",
       " 'group',\n",
       " 'gruelling',\n",
       " 'had',\n",
       " 'halep',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hid',\n",
       " 'highly',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'hodgson',\n",
       " 'hold',\n",
       " 'hope',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'however',\n",
       " 'hubby',\n",
       " 'huge',\n",
       " 'hungry',\n",
       " 'i',\n",
       " 'ice',\n",
       " 'iced',\n",
       " 'if',\n",
       " 'illness',\n",
       " 'important',\n",
       " 'improved',\n",
       " 'in',\n",
       " 'including',\n",
       " 'injury',\n",
       " 'is',\n",
       " 'it',\n",
       " 'italico',\n",
       " 'ivan',\n",
       " 'jelena',\n",
       " 'jonesing',\n",
       " 'jose',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kerber',\n",
       " 'kitchen',\n",
       " 'knock',\n",
       " 'know',\n",
       " 'konta',\n",
       " 'last',\n",
       " 'late',\n",
       " 'laughed',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'led',\n",
       " 'left',\n",
       " 'leicester',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'liverpool',\n",
       " 'located',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'losing',\n",
       " 'losses',\n",
       " 'luka',\n",
       " 'lunchtime',\n",
       " 'madrid',\n",
       " 'major',\n",
       " 'make',\n",
       " 'making',\n",
       " 'manager',\n",
       " 'many',\n",
       " 'match',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'meal',\n",
       " 'miami',\n",
       " 'minutes',\n",
       " 'missed',\n",
       " 'modric',\n",
       " 'moment',\n",
       " 'months',\n",
       " 'montreal',\n",
       " 'montreal.',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'murray',\n",
       " 'my',\n",
       " 'nachos',\n",
       " 'nadal',\n",
       " 'names',\n",
       " 'nederer',\n",
       " 'never',\n",
       " 'new',\n",
       " 'nn7',\n",
       " 'no-quarter-given',\n",
       " 'no1',\n",
       " 'no2',\n",
       " 'no46',\n",
       " 'no83',\n",
       " 'north',\n",
       " 'not',\n",
       " 'odds',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh-so-familiar',\n",
       " 'omelettes',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'ordered',\n",
       " 'orders',\n",
       " 'ostapenko',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'palo',\n",
       " 'pancakes',\n",
       " 'parking',\n",
       " 'pasta',\n",
       " 'patience',\n",
       " 'peak',\n",
       " 'people',\n",
       " 'performances',\n",
       " 'perisic',\n",
       " 'perseyside',\n",
       " 'person',\n",
       " 'pessimistic',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plates',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'pleasantly',\n",
       " 'plus',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'possible',\n",
       " 'potential',\n",
       " 'premier',\n",
       " 'prepare',\n",
       " 'presentation',\n",
       " 'probably',\n",
       " 'promote',\n",
       " 'proved',\n",
       " 'pull',\n",
       " 'pulled',\n",
       " 'quality',\n",
       " 'quarter-final',\n",
       " 'quarter—except',\n",
       " 'quartet',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'rakitic',\n",
       " 'ran',\n",
       " 'ranked',\n",
       " 'ranking',\n",
       " 'rankings',\n",
       " 'ranks',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'real',\n",
       " 'really',\n",
       " 'recommendations',\n",
       " 'recommended',\n",
       " 'reds',\n",
       " 'reigning',\n",
       " 'reservations',\n",
       " 'resignation',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'results',\n",
       " 'return',\n",
       " 'returning',\n",
       " 'reverse',\n",
       " 'rich',\n",
       " 'roma',\n",
       " 'round',\n",
       " 'row',\n",
       " 'roy',\n",
       " 'run-in',\n",
       " 'russia',\n",
       " 'said',\n",
       " 'salty',\n",
       " 'san',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'say',\n",
       " 'scored',\n",
       " 'seal',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seeded',\n",
       " 'seeding',\n",
       " 'seem',\n",
       " 'semi-final',\n",
       " 'semis',\n",
       " 'separate',\n",
       " 'serena',\n",
       " 'served',\n",
       " 'server',\n",
       " 'serves',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'she',\n",
       " 'sheets',\n",
       " 'showed',\n",
       " 'side',\n",
       " 'signed',\n",
       " 'signs',\n",
       " 'simona',\n",
       " 'since',\n",
       " 'sisters',\n",
       " 'sliders',\n",
       " 'slides',\n",
       " 'sloane',\n",
       " 'slow',\n",
       " 'slower',\n",
       " 'slump',\n",
       " 'small',\n",
       " 'smash',\n",
       " 'so',\n",
       " 'soft',\n",
       " 'some',\n",
       " 'soon',\n",
       " 'special)',\n",
       " 'spot',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'started',\n",
       " 'starter',\n",
       " 'stephens',\n",
       " 'straight',\n",
       " 'struggled',\n",
       " 'stunning',\n",
       " 'style',\n",
       " 'sub-par',\n",
       " 'succeed',\n",
       " 'summer',\n",
       " 'sunnyvale',\n",
       " 'surged',\n",
       " 'surprised',\n",
       " 'sweet',\n",
       " 'swing',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'term',\n",
       " 'tests',\n",
       " 'texture',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'thiadal',\n",
       " 'things',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'tie-break',\n",
       " 'time',\n",
       " 'title',\n",
       " 'to',\n",
       " 'toast',\n",
       " 'too',\n",
       " 'took',\n",
       " 'top',\n",
       " 'tough',\n",
       " 'tournament',\n",
       " 'tournaments',\n",
       " 'travel',\n",
       " 'trophy',\n",
       " 'truly',\n",
       " 'turnaround',\n",
       " 'two',\n",
       " 'two-sets',\n",
       " 'undercurrents',\n",
       " 'unless',\n",
       " 'up',\n",
       " 'us',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'victoria',\n",
       " 'victory',\n",
       " 'visit',\n",
       " 'waffle',\n",
       " 'wait',\n",
       " 'waiter',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'was',\n",
       " 'wasnt',\n",
       " 'way',\n",
       " 'we',\n",
       " 'week',\n",
       " 'weekday',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'wild',\n",
       " 'will',\n",
       " 'williams',\n",
       " 'wimbledon',\n",
       " 'win',\n",
       " 'winning',\n",
       " 'wins',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonderful',\n",
       " 'world',\n",
       " 'would',\n",
       " 'wozniacki',\n",
       " 'year',\n",
       " 'years',\n",
       " 'year—and',\n",
       " 'york',\n",
       " 'you',\n",
       " '–'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'burgers'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 인덱스로 바꾸기\n",
    "idx2word = dict(enumerate(results))\n",
    "idx2word\n",
    "\n",
    "# 잘만들어졌는지 확인해보자\n",
    "# 인덱스로 문자 알려주기\n",
    "idx2word[396]\n",
    "idx2word[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuple 형태로 바꾸기 \n",
    "word2idx = {v:k for k, v in idx2word.items()}\n",
    "word2idx\n",
    "\n",
    "# 잘 만들었는지 확인해보기\n",
    "# 문자로 인덱스 알려주기 \n",
    "word2idx[\"bread\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_paragraph(paragraph):\n",
    "    words = paragraph.split(\" \")\n",
    "    encoded = []\n",
    "    for word in words:\n",
    "        encoded.append([word2idx[word]])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def encode_category(category):\n",
    "    if category == \"food\":\n",
    "        return [1, 0]\n",
    "    else:\n",
    "        return [0, 1]\n",
    "\n",
    "# 전체 단어의 개수 몇개인지 세어주는 함수 \n",
    "def word_cnt(paragraph):\n",
    "    return len(paragraph.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 새로 만들기 # 훈련시키려면 문자들 다 숫자로 바꿔야하니까 바꾼거 \n",
    "df[\"enc_paragraph\"] = df.paragraph.apply(encode_paragraph)\n",
    "df[\"enc_category\"] = df.category.apply(encode_category)\n",
    "df[\"seq_length\"] = df.paragraph.apply(word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "      <th>enc_category</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[130], [336], [318], [60], [505], [346], [120...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[36], [482], [478], [150], [174], [210], [161...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[510], [344], [342], [2], [329], [184], [57],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[391], [499], [469], [463], [118], [124], [26...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[124], [359], [57], [512], [328], [506], [111...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[124], [121], [415], [213], [396], [330], [50...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[422], [275], [328], [21], [489], [236], [124...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[106], [489], [494], [312], [261], [54], [360...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[124], [284], [474], [6], [283], [41], [396],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[507], [124], [122], [40], [381], [266], [124...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [382], [235], [55], [145], [323], [521...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [414], [328], [18], [455], [234], [153...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[207], [365], [345], [293], [212], [300], [31...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[190], [101], [474], [456], [491], [68], [60]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [201], [481], [241], [253], [372], [29...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [378], [146], [369], [60], [148], [303...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[341], [151], [433], [160], [456], [267], [32...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[246], [299], [224], [341], [151], [300], [48...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[179], [124], [214], [174], [481], [428], [45...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[7], [474], [124], [159], [104], [58], [518],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  ... enc_category seq_length\n",
       "0   dishplace is located in sunnyvale downtown the...     food  ...       [1, 0]         53\n",
       "1   service can be slower during busy hours but ou...     food  ...       [1, 0]         19\n",
       "2   portions are huge both french toast and their ...     food  ...       [1, 0]         42\n",
       "3   we started with apps going the chicken and waf...     food  ...       [1, 0]         43\n",
       "4   the biscuits and gravy was too salty two peopl...     food  ...       [1, 0]         82\n",
       "5   the garlic fries were a great starter (and a h...     food  ...       [1, 0]         24\n",
       "6   our meal was excellent i had the pasta ai form...     food  ...       [1, 0]         50\n",
       "7   what i enjoy most about palo alto is so many r...     food  ...       [1, 0]         43\n",
       "8   the drinks came out fairly quickly a good two ...     food  ...       [1, 0]         49\n",
       "9   despite the not so good burger the service was...     food  ...       [1, 0]         82\n",
       "10  the four reigning major champions simona halep...   sports  ...       [0, 1]         65\n",
       "11  the briton was seeded nn7 here last year befor...   sports  ...       [0, 1]         88\n",
       "12  stephens surged her way back from injury in st...   sports  ...       [0, 1]         91\n",
       "13  when it came to england chances in the world c...   sports  ...       [0, 1]         71\n",
       "14  the team that eliminated russia – croatia – al...   sports  ...       [0, 1]         70\n",
       "15  the perseyside outfit finished in fourth place...   sports  ...       [0, 1]         30\n",
       "16  liverpool fc will return to premier league act...   sports  ...       [0, 1]         35\n",
       "17  alisson signed for liverpool fc from as roma t...   sports  ...       [0, 1]         30\n",
       "18  but the rankings during that run-in to new yor...   sports  ...       [0, 1]         63\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports  ...       [0, 1]         46\n",
       "\n",
       "[20 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지의 sample 데이터는 길이가 다 같았음\n",
    "\n",
    "길이가 서로 다를 경우에는 우찌?\n",
    "\n",
    "길이가 모자란 애들은 공백으로 처리되어 공백도 하나의 데이터로 분석을 해버리고 성능은 떨어지게 된다\n",
    "\n",
    "그래서 쓰는게 \n",
    "\n",
    "# \n",
    "\n",
    "#### dynamic_rnn : 불규칙한 입력데이터의 길이를 동적으로 처리하는 함수\n",
    "\n",
    "static_rnn은 다 같은 길이로 쓰는 애 \n",
    "\n",
    "그렇기 때문에 dynamic_rnn을 많이 씀\n",
    "\n",
    "#\n",
    "\n",
    "가장 긴 길이를 기준으로 하여 크기를 잡아줌\n",
    "\n",
    "공백이 생기는 애들은 공백에 padding 해줌 즉, 표시 해두기 \n",
    "\n",
    "그럼 dynamic_rnn은 학습을 안해도 되는구나 하고 빼버림\n",
    "\n",
    "그래서 데이터의 전체 길이를 알아야했다요!!!!!!!!!!\n",
    "\n",
    "#\n",
    "\n",
    "그니까 최대값이 무엇인지 알아내는 코드가 필요하다...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# 문장의 최대 길이 알아내기 \n",
    "\n",
    "max_word_cnt = 0\n",
    "for row in df[\"paragraph\"]:\n",
    "    if len(row.split(\" \")) > max_word_cnt:\n",
    "        max_word_cnt = len(row.split(\" \"))\n",
    "        \n",
    "print(max_word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 길이가 91개가 안될 경우 -1로 패딩 처리\n",
    "def sequence_padding(enc_paragraph):\n",
    "    seq_length = len(enc_paragraph)\n",
    "    for i in range(seq_length, max_word_cnt):\n",
    "        enc_paragraph.append([-1])\n",
    "        \n",
    "    return enc_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enc_paragraph\"] = df.enc_paragraph.apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "      <th>enc_category</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[130], [336], [318], [60], [505], [346], [120...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[36], [482], [478], [150], [174], [210], [161...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[510], [344], [342], [2], [329], [184], [57],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[391], [499], [469], [463], [118], [124], [26...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[124], [359], [57], [512], [328], [506], [111...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[124], [121], [415], [213], [396], [330], [50...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[422], [275], [328], [21], [489], [236], [124...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[106], [489], [494], [312], [261], [54], [360...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[124], [284], [474], [6], [283], [41], [396],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[507], [124], [122], [40], [381], [266], [124...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [382], [235], [55], [145], [323], [521...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [414], [328], [18], [455], [234], [153...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[207], [365], [345], [293], [212], [300], [31...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[190], [101], [474], [456], [491], [68], [60]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [201], [481], [241], [253], [372], [29...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[124], [378], [146], [369], [60], [148], [303...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[341], [151], [433], [160], [456], [267], [32...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[246], [299], [224], [341], [151], [300], [48...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[179], [124], [214], [174], [481], [428], [45...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[7], [474], [124], [159], [104], [58], [518],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  ... enc_category seq_length\n",
       "0   dishplace is located in sunnyvale downtown the...     food  ...       [1, 0]         53\n",
       "1   service can be slower during busy hours but ou...     food  ...       [1, 0]         19\n",
       "2   portions are huge both french toast and their ...     food  ...       [1, 0]         42\n",
       "3   we started with apps going the chicken and waf...     food  ...       [1, 0]         43\n",
       "4   the biscuits and gravy was too salty two peopl...     food  ...       [1, 0]         82\n",
       "5   the garlic fries were a great starter (and a h...     food  ...       [1, 0]         24\n",
       "6   our meal was excellent i had the pasta ai form...     food  ...       [1, 0]         50\n",
       "7   what i enjoy most about palo alto is so many r...     food  ...       [1, 0]         43\n",
       "8   the drinks came out fairly quickly a good two ...     food  ...       [1, 0]         49\n",
       "9   despite the not so good burger the service was...     food  ...       [1, 0]         82\n",
       "10  the four reigning major champions simona halep...   sports  ...       [0, 1]         65\n",
       "11  the briton was seeded nn7 here last year befor...   sports  ...       [0, 1]         88\n",
       "12  stephens surged her way back from injury in st...   sports  ...       [0, 1]         91\n",
       "13  when it came to england chances in the world c...   sports  ...       [0, 1]         71\n",
       "14  the team that eliminated russia – croatia – al...   sports  ...       [0, 1]         70\n",
       "15  the perseyside outfit finished in fourth place...   sports  ...       [0, 1]         30\n",
       "16  liverpool fc will return to premier league act...   sports  ...       [0, 1]         35\n",
       "17  alisson signed for liverpool fc from as roma t...   sports  ...       [0, 1]         30\n",
       "18  but the rankings during that run-in to new yor...   sports  ...       [0, 1]         63\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports  ...       [0, 1]         46\n",
       "\n",
       "[20 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130], [336], [318], [60], [505], [346], [120], [336], [375], [171], [124], [164], [179], [101], [482], [478], [525], [456], [447], [174], [468], [73], [161], [430], [37], [57], [489], [474], [456], [27], [303], [224], [134], [389], [396], [43], [519], [213], [347], [210], [40], [489], [185], [35], [279], [66], [377], [127], [181], [124], [479], [456], [442], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"enc_paragraph\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값들을 배열로 변환\n",
    "enc_paragraph = np.array(df.enc_paragraph.tolist())\n",
    "enc_category = np.array(df.enc_category.tolist())\n",
    "seq_length = np.array(df.seq_length.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = enc_paragraph\n",
    "train_y = enc_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 91, 1)\n",
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 모델 구축\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, max_word_cnt, 1])\n",
    "y = tf.placeholder(tf.int32, [None, 2])\n",
    "\n",
    "embedding = tf.layers.dense(X, 5)\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "dense_layer = tf.layers.dense(state.h, 32)\n",
    "logits = tf.layers.dense(dense_layer, 2)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 91, 1), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 91, 5), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 64) dtype=float32>)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(embedding)\n",
    "print(state)\n",
    "print(dense_layer)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.8909272, accuracy:0.5\n",
      "epoch:50, loss:0.45044193, accuracy:0.75\n",
      "epoch:100, loss:0.15099776, accuracy:0.95\n",
      "epoch:150, loss:0.02503803, accuracy:1.0\n",
      "epoch:200, loss:0.0032332118, accuracy:1.0\n",
      "epoch:250, loss:0.00103654, accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        _, cost = sess.run([train, loss], feed_dict={X:train_X, y:train_y})\n",
    "        if epoch % 50 == 0:\n",
    "            pred = tf.nn.softmax(logits)\n",
    "            correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "            cur_acc = accuracy.eval({X:train_X, y:train_y})\n",
    "            print(\"epoch:\" + str(epoch) + \", loss:\" + str(cost) + \", accuracy:\" + str(cur_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
